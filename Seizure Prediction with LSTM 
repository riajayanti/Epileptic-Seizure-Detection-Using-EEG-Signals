{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riajayanti/Epileptic-Seizure-Detection-and-Prediction-Using-EEG-Signals/blob/main/Seizure%20Prediction%20with%20LSTM%20\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loSh7tX69CWq"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwUvFyg09CWs",
        "outputId": "f74e4e9b-b96f-469b-f631-1509a99220e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.9.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.5)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.1.31)\n",
            "Downloading mne-1.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mne # Install the 'mne' libraryimport numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, LSTM, Dense, Dropout, concatenate,\n",
        "    BatchNormalization, Bidirectional\n",
        ")\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ModelCheckpoint,\n",
        "    ReduceLROnPlateau, TensorBoard\n",
        ")\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import mne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KXuyuuJ9CWx"
      },
      "source": [
        "## 2. Model Architecture\n",
        "\n",
        "We'll create a hybrid model that combines sequence data with engineered features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM6pukxP9CWy"
      },
      "outputs": [],
      "source": [
        "class SeizurePredictor:\n",
        "    def __init__(self,\n",
        "                 sequence_shape,\n",
        "                 feature_shape,\n",
        "                 lstm_units=[64, 32], # Can try changing num_units\n",
        "                 dense_units=[32], # Can try changing num_units\n",
        "                 dropout_rate=0.3):\n",
        "        \"\"\"Initialize seizure prediction model\n",
        "\n",
        "        Args:\n",
        "            sequence_shape: Shape of EEG sequences (seq_len, n_channels, n_times)\n",
        "            feature_shape: Shape of engineered features\n",
        "            lstm_units: List of units in LSTM layers\n",
        "            dense_units: List of units in Dense layers\n",
        "            dropout_rate: Dropout rate for regularization\n",
        "        \"\"\"\n",
        "        self.sequence_shape = sequence_shape\n",
        "        self.feature_shape = feature_shape\n",
        "        self.lstm_units = lstm_units\n",
        "        self.dense_units = dense_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Build the hybrid LSTM model\"\"\"\n",
        "        # Sequence input branch\n",
        "        seq_input = Input(shape=self.sequence_shape, name='sequence_input')\n",
        "        x = seq_input\n",
        "\n",
        "        # LSTM layers\n",
        "        for i, units in enumerate(self.lstm_units):\n",
        "            return_sequences = i < len(self.lstm_units) - 1\n",
        "            x = Bidirectional(LSTM(units, return_sequences=return_sequences))(x) # Can add more BiDirectional Layers\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Dropout(self.dropout_rate)(x)\n",
        "\n",
        "        # Feature input branch\n",
        "        feature_input = Input(shape=self.feature_shape, name='feature_input')\n",
        "\n",
        "        # Combine branches\n",
        "        combined = concatenate([x, feature_input])\n",
        "\n",
        "        # Dense layers\n",
        "        for units in self.dense_units:\n",
        "            combined = Dense(units, activation='relu')(combined) # Can add more dense layers here\n",
        "            combined = BatchNormalization()(combined) # 1 for each dense layer\n",
        "            combined = Dropout(self.dropout_rate)(combined)\n",
        "\n",
        "\n",
        "        # Output layer\n",
        "        output = Dense(1, activation='sigmoid')(combined)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs=[seq_input, feature_input], outputs=output)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, learning_rate=0.001, class_weights=None):\n",
        "        \"\"\"Compile the model with custom loss and metrics\n",
        "\n",
        "        Args:\n",
        "            learning_rate: Initial learning rate\n",
        "            class_weights: Weights for different classes\n",
        "        \"\"\"\n",
        "        # Custom focal loss to handle class imbalance\n",
        "        def focal_loss(gamma=2., alpha=.25):\n",
        "            def focal_loss_fixed(y_true, y_pred):\n",
        "                pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "                pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "                return -tf.reduce_mean(\n",
        "                    alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1 + 1e-7) +\n",
        "                    (1-alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0 + 1e-7)\n",
        "                )\n",
        "            return focal_loss_fixed\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=focal_loss(),\n",
        "            metrics=['accuracy',\n",
        "                    tf.keras.metrics.Precision(),\n",
        "                    tf.keras.metrics.Recall(),\n",
        "                    AUC()]\n",
        "        )\n",
        "\n",
        "    def create_callbacks(self, patience=10):\n",
        "        \"\"\"Create training callbacks\n",
        "\n",
        "        Args:\n",
        "            patience: Number of epochs to wait for improvement\n",
        "        \"\"\"\n",
        "        # Create log directory for tensorboard\n",
        "        log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=patience,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                'best_model.h5',\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=patience//2,\n",
        "                min_lr=1e-6\n",
        "            ),\n",
        "            TensorBoard(log_dir=log_dir)\n",
        "        ]\n",
        "\n",
        "        return callbacks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zRoXVWD9CW1"
      },
      "source": [
        "## 3. Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAmJp1iuXH6M"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcNrXAmW9CW3"
      },
      "outputs": [],
      "source": [
        "class TrainingPipeline:\n",
        "    def __init__(self, predictor):\n",
        "        \"\"\"Initialize training pipeline\n",
        "\n",
        "        Args:\n",
        "            predictor: Instance of SeizurePredictor\n",
        "        \"\"\"\n",
        "        self.predictor = predictor\n",
        "\n",
        "    def train(self,\n",
        "              X_train_seq, X_train_features, y_train,\n",
        "              X_val_seq, X_val_features, y_val,\n",
        "              batch_size=32,\n",
        "              epochs=100,\n",
        "              class_weights=None):\n",
        "        \"\"\"Train the model\n",
        "\n",
        "        Args:\n",
        "            X_train_seq: Training sequences\n",
        "            X_train_features: Training features\n",
        "            y_train: Training labels\n",
        "            X_val_seq: Validation sequences\n",
        "            X_val_features: Validation features\n",
        "            y_val: Validation labels\n",
        "        \"\"\"\n",
        "        # Compile model\n",
        "        self.predictor.compile_model(class_weights=class_weights)\n",
        "\n",
        "        # Get callbacks\n",
        "        callbacks = self.predictor.create_callbacks()\n",
        "\n",
        "        # Train model\n",
        "        history = self.predictor.model.fit(\n",
        "            [X_train_seq, X_train_features],\n",
        "            y_train,\n",
        "            validation_data=([X_val_seq, X_val_features], y_val),\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, X_test_seq, X_test_features, y_test):\n",
        "        \"\"\"Evaluate the model\n",
        "\n",
        "        Args:\n",
        "            X_test_seq: Test sequences\n",
        "            X_test_features: Test features\n",
        "            y_test: Test labels\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred_prob = self.predictor.model.predict([X_test_seq, X_test_features])\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(\n",
        "            confusion_matrix(y_test, y_pred),\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues'\n",
        "        )\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "        return y_pred_prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI171xji9CW6"
      },
      "source": [
        "## 4. Usage Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI93mAGoMjZ7",
        "outputId": "15926732-34a7-44d8-931b-2f680e96b59f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9wEkKRL9CW7",
        "outputId": "bc0280af-a547-4d79-e3e1-c923be75d71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting EDF parameters from /content/drive/Shareddrives/riajayanti/physionet.org/files/chbmit/1.0.0/chb02/chb02_01.edf...\n",
            "EDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 921599  =      0.000 ...  3599.996 secs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-84f919b3d1b7>:14: RuntimeWarning: Channel names are not unique, found duplicates for: {'T8-P8'}. Applying running numbers for duplicates.\n",
            "  raw = mne.io.read_raw_edf(file_path, preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
            "Original class distribution: Counter({0.0: 3500, 1.0: 100})\n",
            "Balanced class distribution: Counter({0.0: 3500, 1.0: 2450})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 600ms/step - accuracy: 0.5666 - loss: 0.6817 - val_accuracy: 0.5776 - val_loss: 0.6647\n",
            "Epoch 2/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 457ms/step - accuracy: 0.5724 - loss: 0.6575 - val_accuracy: 0.5877 - val_loss: 0.6492\n",
            "Epoch 3/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 478ms/step - accuracy: 0.5786 - loss: 0.6399 - val_accuracy: 0.5647 - val_loss: 0.6378\n",
            "Epoch 4/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 475ms/step - accuracy: 0.6069 - loss: 0.6294 - val_accuracy: 0.6364 - val_loss: 0.6134\n",
            "Epoch 5/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 478ms/step - accuracy: 0.6623 - loss: 0.6014 - val_accuracy: 0.6790 - val_loss: 0.5953\n",
            "Epoch 6/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 475ms/step - accuracy: 0.6701 - loss: 0.5918 - val_accuracy: 0.7014 - val_loss: 0.5819\n",
            "Epoch 7/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 475ms/step - accuracy: 0.6859 - loss: 0.5840 - val_accuracy: 0.7143 - val_loss: 0.5813\n",
            "Epoch 8/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 471ms/step - accuracy: 0.6406 - loss: 0.6097 - val_accuracy: 0.6655 - val_loss: 0.6025\n",
            "Epoch 9/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 473ms/step - accuracy: 0.6567 - loss: 0.6033 - val_accuracy: 0.6919 - val_loss: 0.5800\n",
            "Epoch 10/10\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 478ms/step - accuracy: 0.6842 - loss: 0.5796 - val_accuracy: 0.6056 - val_loss: 0.6420\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 133ms/step\n",
            "Accuracy: 0.6919\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.63      0.71      1050\n",
            "         1.0       0.60      0.78      0.68       735\n",
            "\n",
            "    accuracy                           0.69      1785\n",
            "   macro avg       0.70      0.71      0.69      1785\n",
            "weighted avg       0.72      0.69      0.69      1785\n",
            "\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step\n",
            "ROC-AUC: 0.7645\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import mne\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Function to preprocess raw EEG data into epochs\n",
        "def preprocess_raw_data(file_path, selected_channels, epoch_length=1):\n",
        "    raw = mne.io.read_raw_edf(file_path, preload=True)\n",
        "    raw.pick_channels(selected_channels)\n",
        "    sampling_rate = int(raw.info['sfreq'])\n",
        "    samples_per_epoch = epoch_length * sampling_rate\n",
        "\n",
        "    data, _ = raw[:]\n",
        "    num_samples = data.shape[1]\n",
        "    epochs = [data[:, i:i+samples_per_epoch] for i in range(0, num_samples, samples_per_epoch) if i+samples_per_epoch <= num_samples]\n",
        "\n",
        "    return np.array(epochs)\n",
        "\n",
        "# Function to create labels from seizure annotations\n",
        "def create_labels_from_annotations(epochs, annotations, sampling_rate=256, epoch_length=1):\n",
        "    num_epochs = len(epochs)\n",
        "    samples_per_epoch = sampling_rate * epoch_length\n",
        "    labels = np.zeros(num_epochs)\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        epoch_start_time = (i * samples_per_epoch) / sampling_rate\n",
        "        epoch_end_time = epoch_start_time + epoch_length\n",
        "\n",
        "        for annotation in annotations:\n",
        "            if (epoch_start_time < annotation[\"end_time\"]) and (epoch_end_time > annotation[\"start_time\"]):\n",
        "                labels[i] = 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "# Function to balance data using SMOTE\n",
        "def balance_seizure_data(epochs_array, labels):\n",
        "    n_epochs, n_channels, n_timepoints = epochs_array.shape\n",
        "    X_reshaped = epochs_array.reshape(n_epochs, n_channels * n_timepoints)\n",
        "\n",
        "    smote = SMOTE(sampling_strategy=0.7)  # Increased the SMOTE ratio\n",
        "    X_balanced, y_balanced = smote.fit_resample(X_reshaped, labels)\n",
        "\n",
        "    X_balanced_3d = X_balanced.reshape(-1, n_channels, n_timepoints)\n",
        "    print(\"Original class distribution:\", Counter(labels))\n",
        "    print(\"Balanced class distribution:\", Counter(y_balanced))\n",
        "    return X_balanced_3d, y_balanced\n",
        "\n",
        "# Function to validate balanced data\n",
        "def validate_balanced_data(X_balanced, y_balanced):\n",
        "    if np.any(np.isnan(X_balanced)) or np.any(np.isinf(X_balanced)):\n",
        "        raise ValueError(\"Dataset contains NaN or infinite values\")\n",
        "    class_counts = Counter(y_balanced)\n",
        "    ratio = min(class_counts.values()) / max(class_counts.values())\n",
        "    if ratio < 0.4:\n",
        "        print(\"Warning: Classes still significantly imbalanced\")\n",
        "\n",
        "# Main Script\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/content/drive/Shareddrives/riajayanti/physionet.org/files/chbmit/1.0.0/chb02/chb02_01.edf'\n",
        "    seizure_annotations = [\n",
        "        {\"start_time\": 500, \"end_time\": 560},\n",
        "        {\"start_time\": 1000, \"end_time\": 1040},\n",
        "    ]\n",
        "\n",
        "    selected_channels = ['FP1-F7', 'FP2-F8', 'F7-T7', 'F8-T8', 'FZ-CZ', 'CZ-PZ']\n",
        "    epochs = preprocess_raw_data(file_path, selected_channels, epoch_length=1)\n",
        "    ictal_labels = create_labels_from_annotations(epochs, seizure_annotations)\n",
        "\n",
        "    # Normalize epochs\n",
        "    epochs_normalized = epochs / np.max(np.abs(epochs), axis=(1, 2), keepdims=True)\n",
        "\n",
        "    # Balance the data using SMOTE\n",
        "    X_balanced, y_balanced = balance_seizure_data(epochs_normalized, ictal_labels)\n",
        "    validate_balanced_data(X_balanced, y_balanced)\n",
        "\n",
        "    # Reshape for LSTM compatibility\n",
        "    X_seq = X_balanced.transpose(0, 2, 1)\n",
        "\n",
        "    # Split into train and test sets with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_seq, y_balanced, test_size=0.3, random_state=123, stratify=y_balanced\n",
        "    )\n",
        "\n",
        "    # Build the LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))  # LSTM units = 128; return_sequences = False\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Single output for binary classification\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])  # Learning rate = 0.0005\n",
        "\n",
        "    # Set up EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)  # Patience = 4\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])  # Epochs = 10; Batch size = 64\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    auc = roc_auc_score(y_test, model.predict(X_test))\n",
        "    print(f'ROC-AUC: {auc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZS74KyD9CW_"
      },
      "source": [
        "## 5. Exercises for Students\n",
        "\n",
        "1. **Model Architecture**:\n",
        "   - Try different LSTM architectures (stacked, residual connections)\n",
        "   - Experiment with different numbers of layers and units\n",
        "   - Implement attention mechanisms\n",
        "   - Add regularization techniques\n",
        "\n",
        "2. **Loss Functions**:\n",
        "   - Implement custom loss functions that penalize late predictions\n",
        "   - Try different focal loss parameters\n",
        "   - Create a loss function that considers prediction timing\n",
        "\n",
        "3. **Evaluation Metrics**:\n",
        "   - Implement clinically relevant metrics (e.g., prediction horizon)\n",
        "   - Create visualization tools for false alarms\n",
        "   - Add statistical tests for model comparison\n",
        "\n",
        "4. **Model Optimization**:\n",
        "   - Implement cross-validation\n",
        "   - Try different optimizers and learning rate schedules\n",
        "   - Add model ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYBFxbnO9CXA"
      },
      "source": [
        "## 6. Next Steps\n",
        "\n",
        "1. Test model on longer continuous recordings\n",
        "2. Evaluate performance across different patients\n",
        "3. Implement real-time prediction pipeline\n",
        "4. Consider deployment requirements\n",
        "5. Add visualization tools for clinical interpretation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}